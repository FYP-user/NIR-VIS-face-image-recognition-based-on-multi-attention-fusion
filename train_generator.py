import os
import argparse
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import torch.backends.cudnn as cudnn
import torchvision.utils as vutils

from utils import *
from network.ID_net import define_ID
from network.G_net import define_G
from network.lightcnn import LightCNN_29v2
from data.dataset import Dataset


parser = argparse.ArgumentParser()           #åˆ›å»ºè§£æå™¨

parser.add_argument('--gpu_ids', default='0', type=str)
parser.add_argument('--workers', default=8, type=int)
parser.add_argument('--lr', default=0.0002, type=float)
parser.add_argument('--batch_size', default=4, type=int)
parser.add_argument('--epochs', default=5, type=int)
parser.add_argument('--pre_epoch', default=0, type=int, help='train from previous model')

parser.add_argument('--print_iter', default=20, type=int, help='print frequency')
parser.add_argument('--save_epoch', default=1, type=int)
parser.add_argument('--output_path', default='./results', type=str)

parser.add_argument('--weights_lightcnn', default='./pre_train/LightCNN_29Layers_V2_checkpoint.pth.tar', type=str)
parser.add_argument('--weights_dec', default='./pre_train/dec_epoch_45.pth.tar', type=str, help='dec is the identity sampler')
parser.add_argument('--img_root',  default='dataset/CASIA_2.0/', type=str)
parser.add_argument('--train_list', default='dataset/CASIA_2.0/together.csv', type=str)


def main():
    global args                                                                    #argså…¨å±€å˜é‡
    args = parser.parse_args()                                                     #æŠŠparserä¸­è®¾ç½®çš„æ‰€æœ‰"add_argument"ç»™è¿”å›åˆ°argså­ç±»å®ä¾‹å½“ä¸­
    print(args)

    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_ids                              #æŒ‡å®šè¦ä½¿ç”¨çš„æ˜¾å¡
    cudnn.benchmark = True                                                         #å¢åŠ ç¨‹åºçš„è¿è¡Œæ•ˆç‡

    if not os.path.exists(args.output_path):                                       #åˆ¤æ–­æ‹¬å·é‡Œçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç›®å½•
        os.makedirs(args.output_path)

    # lightcnn(é¢„è®­ç»ƒè¯†åˆ«ç½‘ç»œF)
    LightCNN = LightCNN_29v2(is_train=False)
    print("=> loading pretrained lightcnn '{}'".format(args.weights_lightcnn))
    load_model(LightCNN, args.weights_lightcnn)                                     #è½½å…¥æ¨¡å‹ä»¥åŠç›¸å…³æƒé‡å‚æ•°
    set_requires_grad([LightCNN], False)                           #ä¸éœ€è¦ä¸ºè¿™ä¸ª[LightCNN]è®¡ç®—æ¢¯åº¦
    LightCNN.eval()                                                   #è¿”å›ä¼ å…¥å­—ç¬¦ä¸²çš„è¡¨è¾¾å¼çš„ç»“æœã€‚å°±æ˜¯è¯´ï¼šå°†å­—ç¬¦ä¸²å½“æˆæœ‰æ•ˆçš„è¡¨è¾¾å¼æ¥æ±‚å€¼å¹¶è¿”å›è®¡ç®—ç»“æœ

    # id sampler è·å–VISçš„èº«ä»½ä¿¡æ¯
    dec = define_ID()                                                                #dec:èº«ä»½é‡‡æ ·å™¨(Fs)
    print("=> loading pretrained identity sampler '{}'".format(args.weights_dec))
    load_model(dec, args.weights_dec)                                                #è½½å…¥æ¨¡å‹ä»¥åŠç›¸å…³æƒé‡å‚æ•°
    set_requires_grad([dec], False)                                 #ä¸éœ€è¦ä¸ºè¿™ä¸ª[dec]å¼ é‡è®¡ç®—æ¢¯åº¦
    dec.eval()                                                        #è¿”å›ä¼ å…¥å­—ç¬¦ä¸²çš„è¡¨è¾¾å¼çš„ç»“æœã€‚å°±æ˜¯è¯´ï¼šå°†å­—ç¬¦ä¸²å½“æˆæœ‰æ•ˆçš„è¡¨è¾¾å¼æ¥æ±‚å€¼å¹¶è¿”å›è®¡ç®—ç»“æœ

    # generator(å¯¹å¶å˜åˆ†å‘ç”Ÿå™¨)
    encoder_nir, encoder_vis, decoder = define_G(input_dim=3, output_dim=3, ndf=32)

    # load pretrained model# è½½å…¥é¢„è®­ç»ƒçš„æ¨¡å‹
    if args.pre_epoch:
        print("load pretrained model %d" % args.pre_epoch)
        load_model(encoder_nir, "./model/encoder_nir_epoch_%d.pth.tar" % args.pre_epoch)       #è½½å…¥ encoder_nir æ¨¡å‹ä»¥åŠç›¸å…³æƒé‡å‚æ•°
        load_model(encoder_vis, "./model/encoder_vis_epoch_%d.pth.tar" % args.pre_epoch)       #è½½å…¥ encoder_vis æ¨¡å‹ä»¥åŠç›¸å…³æƒé‡å‚æ•°
        load_model(decoder, "./model/decoder_epoch_%d.pth.tar" % args.pre_epoch)               #è½½å…¥ decoder æ¨¡å‹ä»¥åŠç›¸å…³æƒé‡å‚æ•°

    # dataset   è·å–é…å¯¹çš„å¼‚æ„äººè„¸æ•°æ®ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”¨è¿™äº›æ•°æ®æ¥è¿›è¡Œè®­ç»ƒ
    train_loader = torch.utils.data.DataLoader(
        Dataset(args), batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)

    # optimizer# æ„é€ ä¸€ä¸ªä¼˜åŒ–å™¨å¯¹è±¡optimizerï¼Œç”¨æ¥ä¿å­˜å½“å‰çš„çŠ¶æ€ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ¥æ›´æ–°å‚æ•°
    # parameters()ä¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆè¿­ä»£å™¨ï¼‰ï¼Œç”Ÿæˆå™¨æ¯æ¬¡ç”Ÿæˆçš„æ˜¯Tensorç±»å‹çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®éƒ½æ˜¯æ¨¡å‹çš„å‚æ•°
    optimizer = optim.Adam(list(encoder_nir.parameters()) + list(encoder_vis.parameters()) +      #lr:å­¦ä¹ ç‡
                           list(decoder.parameters()), lr=args.lr, betas=(0.5, 0.999))            #betas:æƒé‡è¡°å‡

    # criterion
    criterionPix = torch.nn.L1Loss().cuda()                                                     #ç»å¯¹å€¼è¯¯å·®æŸå¤±å‡½æ•°ï¼Œå³L1æŸå¤±å‡½æ•°

    # train  # è®­ç»ƒå¯¹å¶å˜åˆ†å‘ç”Ÿå™¨
    start_epoch = args.pre_epoch + 1
    for epoch in range(start_epoch, args.epochs + 1):                         # 1 <= epoch < 6,æ€»å…±5ä¸ªepoch

        # creat random index  # ç¼–åˆ¶éšæœºæŒ‡æ•°
        arange = torch.arange(args.batch_size).cuda()                         #è¿”å›ä¸€ä¸ªä¸€ç»´å‘é‡,å…¶å¤§å°ä¸º args.batch_size å³[0,1,2,3]
        idx = torch.randperm(args.batch_size).cuda()                          #å°† 0~args.batch_sizeï¼ˆ0,1,2,3ï¼‰éšæœºæ‰“ä¹±åè·å¾—çš„æ•°å­—åºåˆ—
        while 0.0 in (idx - arange):                                          # arange ä¸ idx çš„æ¯ä¸ªå¯¹åº”ä½ä¸èƒ½ç›¸åŒ
            idx = torch.randperm(args.batch_size).cuda()

        for iteration, data in enumerate(train_loader, start=1):              #iteration:è®­ç»ƒæ•°æ®çš„ç´¢å¼•ï¼›data:è®­ç»ƒçš„æ•°æ®
            # get data
            # Variableæ˜¯ä¸€ç§å¯ä»¥ä¸æ–­å˜åŒ–çš„å˜é‡ï¼Œç¬¦åˆåå‘ä¼ æ’­ï¼Œå‚æ•°æ›´æ–°çš„å±æ€§ã€‚pytorchçš„Variableæ˜¯ä¸€ä¸ªå­˜æ”¾ä¼šå˜åŒ–å€¼çš„åœ°ç†ä½ç½®ï¼Œ
            # é‡Œé¢çš„å€¼ä¼šä¸åœå˜åŒ–ï¼Œpytorchéƒ½æ˜¯ç”±tensorè®¡ç®—çš„ï¼Œè€Œtensoré‡Œé¢çš„å‚æ•°æ˜¯Variableå½¢å¼
            nir = Variable(data["NIR"].cuda())                                #è¿™é‡Œ NIR æ˜¯NIRå±æ€§æ•°æ®çš„Variableå‚æ•°å½¢å¼
            vis = Variable(data["VIS"].cuda())                                #è¿™é‡Œ VIS æ˜¯VISå±æ€§æ•°æ®çš„Variableå‚æ•°å½¢å¼

            batch_size = nir.size(0)                                          # batch_size ä¸º NIR çš„è¡Œæ•°
            if batch_size < args.batch_size:                                  # args.batch_size ä¸º4
                continue

            id_vis = LightCNN(rgb2gray(vis))                                  #è·å–NIR-VISçš„èº«ä»½è¡¨ç¤º(ğ‘“)
            noise = torch.zeros(batch_size, 256).normal_(0, 1).cuda()   #torch.zeros():è¿”å›ä¸€ä¸ªå½¢çŠ¶ä¸º(batch_size, 256)ï¼Œé‡Œé¢çš„æ¯ä¸€ä¸ªå€¼éƒ½æ˜¯0çš„tensor
                                                                        # normal(0, 1):è¯¥å‡½æ•°è¿”å›ä»å•ç‹¬çš„æ­£æ€åˆ†å¸ƒä¸­æå–çš„éšæœºæ•°çš„å¼ é‡
            id_noise = dec(noise)                                       #id_noise:ä»å¤§é‡æœªé…å¯¹çš„VISå›¾åƒæå–çš„èº«ä»½è¡¨ç¤º(ğ‘“ Ìƒ)

            # forward
            z_nir = encoder_nir(nir, "enc")                             #z_nir:è¿‘çº¢å¤–å›¾åƒçš„å±æ€§åˆ†å¸ƒ(Zn)
            z_vis = encoder_vis(vis, "enc")                             #z_vis:å¯è§å…‰å›¾åƒçš„å±æ€§åˆ†å¸ƒ(Zv)

            style_nir = encoder_nir(z_nir, "style")                     #style_nir:NIRå›¾åƒçš„é£æ ¼
            style_vis = encoder_vis(z_vis, "style")
            # style_vis:VISå›¾åƒçš„é£æ ¼

            assign_adain_params(style_nir, decoder)                                          #é£æ ¼è¿ç§»(NIRé£æ ¼)
            rec_nir = decoder(torch.cat([id_vis, z_nir], dim=1), "NIR")              #rec_nir:é‡æ„çš„NIRå›¾åƒ(ğ¼ Ì‚ğ‘)
                                                                                        #torch.cat():æŠŠå¤šä¸ªtensorè¿›è¡Œæ‹¼æ¥
            rec_nir_idx = decoder(torch.cat([id_vis[idx, :], z_nir], dim=1), "NIR")   #rec_nir_idx:é€‰æ‹©æ€§è·å–æ•°æ®é‡æ„å›¾åƒ
            fake_nir = decoder(torch.cat([id_noise, z_nir], dim=1), "NIR")            #fake_nir:ç”Ÿæˆçš„NIRå›¾åƒ(ğ¼ Ìƒğ‘)

            assign_adain_params(style_vis, decoder)                                          #é£æ ¼è¿ç§»(VISé£æ ¼)
            rec_vis = decoder(torch.cat([id_vis, z_vis], dim=1), "VIS")               #rec_vis:é‡æ„çš„VISå›¾åƒ(ğ¼ Ì‚ğ‘‰)
            rec_vis_idx = decoder(torch.cat([id_vis[idx, :], z_vis], dim=1), "VIS")   #rec_vis_idx:é€‰æ‹©æ€§è·å–æ•°æ®é‡æ„å›¾åƒ
            fake_vis = decoder(torch.cat([id_noise, z_vis], dim=1), "VIS")            #fake_vis:ç”Ÿæˆçš„VISå›¾åƒ(ğ¼ Ìƒğ‘‰)

            # orthogonal loss  # è§’æ­£äº¤æŸå¤±å‡½æ•°
            loss_ort = 50 * (ort_loss(z_nir, id_vis) + ort_loss(z_vis, id_vis))             #å¼(2)

            # pixel loss       # åˆ†å¸ƒå­¦ä¹ æŸå¤±å‡½æ•°
            loss_pix = 100 * ((criterionPix(rec_nir, nir) + criterionPix(rec_vis, vis)) +           #å¼(4)
                               0.1 * (criterionPix(rec_nir_idx, nir) + criterionPix(rec_vis_idx, vis)) +
                               0.1 * (criterionPix(fake_nir, nir) + criterionPix(fake_vis, vis)))   #å¼(13)

            # identity preserving loss  # æˆå¯¹æ’ç­‰ä¿æŒæŸå¤±
            id_nir_rec = LightCNN(rgb2gray(rec_nir))                          #id_nir_rec:é‡æ„NIRå›¾åƒçš„èº«ä»½è¡¨ç¤º(ğ‘“ Ì‚ğ‘)
            id_vis_rec = LightCNN(rgb2gray(rec_vis))                          #id_vis_rec:é‡æ„VISå›¾åƒçš„èº«ä»½è¡¨ç¤º(ğ‘“ Ì‚ğ‘‰)
            id_nir_fake = LightCNN(rgb2gray(fake_nir))                        #id_nir_fake:ç”Ÿæˆçš„NIRå›¾åƒçš„èº«ä»½è¡¨ç¤º(ğ‘“ Ìƒğ‘)
            id_vis_fake = LightCNN(rgb2gray(fake_vis))                        #id_vis_fake:ç”Ÿæˆçš„VISå›¾åƒçš„èº«ä»½è¡¨ç¤º(ğ‘“ Ìƒğ‘‰)

            real_ang_rec = ang_loss(id_nir_rec, id_vis) + ang_loss(id_vis_rec, id_vis)         #å¼(7),ä½¿è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®š
            real_ang_pair = ang_loss(id_nir_rec, id_vis_rec)                                   #å¼(6),é‡æ„çš„ä¸€å¯¹å›¾åƒèº«ä»½ä¸€è‡´æ€§


            fake_ang_rec = ang_loss(id_nir_fake, id_noise) + ang_loss(id_vis_fake, id_noise)   #å¼(13),ç”Ÿæˆçš„å›¾åƒèº«ä»½ä¸€è‡´æ€§
            fake_ang_pair = ang_loss(id_nir_fake, id_vis_fake)                                 #å¼(14),ç”Ÿæˆçš„ä¸€å¯¹å›¾åƒèº«ä»½ä¸€è‡´æ€§

            loss_ip = - 0.1 * (real_ang_rec + 0.05 * real_ang_pair + fake_ang_rec + 0.05 * fake_ang_pair)

            # all losses # æ€»ä½“æŸå¤±
            loss = loss_ort + loss_pix + loss_ip

            optimizer.zero_grad()           #å°†æ¢¯åº¦å½’é›¶
            loss.backward()                 #åå‘ä¼ æ’­è®¡ç®—å¾—åˆ°æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å€¼
            optimizer.step()                #é€šè¿‡æ¢¯åº¦ä¸‹é™æ‰§è¡Œä¸€æ­¥å‚æ•°æ›´æ–°

            # print log    # è¾“å‡ºæ—¥å¿—
            if iteration % args.print_iter == 0:
                info = "====> Epoch[{}][{}/{}] | ".format(epoch, iteration, len(train_loader))
                info += "Loss: pix: {:4.2f} ort: {:4.2f} | Ang-real rec: {:4.2f} pair: {:4.2f} | Ang-fake rec: {:4.2f} pair: {:4.2f}".format(
                    loss_pix.item(), loss_ort.item(), real_ang_rec.item(), real_ang_pair.item(), fake_ang_rec.item(), fake_ang_pair.item())
                print(info)

            # save images   # å­˜å‚¨å›¾ç‰‡
            if iteration % 500 == 0:
                vutils.save_image(torch.cat([nir, rec_nir, rec_nir_idx, fake_nir, nir[idx, :],
                                             vis, rec_vis, rec_vis_idx, fake_vis, vis[idx, :]], dim=0).data,
                                  "{}/Epoch_{:03d}_Iter_{:06d}_img.png".format(args.output_path, epoch, iteration), nrow=batch_size)

        # save model     # å­˜å‚¨æ¨¡å‹
        if epoch % args.save_epoch == 0:                  #æ¯ä¸ª epoch ç»“æŸå­˜å‚¨æ¨¡å‹
            save_checkpoint(encoder_nir, epoch, "encoder_nir")
            save_checkpoint(encoder_vis, epoch, "encoder_vis")
            save_checkpoint(decoder, epoch, "decoder")




if __name__ == "__main__":
    main()
